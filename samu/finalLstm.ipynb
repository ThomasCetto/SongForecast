{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# maybe final version\n",
    "\n",
    "LSTM that can produce music training with a midi dataset\n",
    "[credits](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#from tqdm.notebook import tqdm, trange\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread\n",
    "from copy import deepcopy\n",
    "import matplotlib as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from music21 import *\n",
    "from np_utils import np_utils\n",
    "from os import listdir\n",
    "import os\n",
    "from collections import Counter\n",
    "import argparse\n",
    "DIR = '../midiFiles'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## caricatore e lettore di dati, direttamente in un dataset pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            sequence_length,\n",
    "            file_limit\n",
    "    ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.notes = self.load_notes(file_limit)\n",
    "        self.uniq_notes = self.get_uniq_notes()\n",
    "\n",
    "        self.index_to_note = {index: note  for index, note in enumerate(self.uniq_notes)}\n",
    "        self.note_to_index = {note: index for index, note in enumerate(self.uniq_notes)}\n",
    "\n",
    "        self.notes_indexes = [self.note_to_index[w] for w in self.notes]\n",
    "\n",
    "    def load_notes(self, file_limit):\n",
    "        out = []\n",
    "        if file_limit is None:\n",
    "            file_limit = len(listdir(DIR))\n",
    "\n",
    "        for file in tqdm(listdir(DIR), total=file_limit):\n",
    "            if not \"mid\" in file:\n",
    "                continue\n",
    "\n",
    "            if file_limit <= 0:\n",
    "                break\n",
    "            else:\n",
    "                file_limit -= 1\n",
    "\n",
    "            midifile = converter.parse(DIR + '/' + file)\n",
    "\n",
    "            #ho solo il piano\n",
    "            notes_to_parse = midifile.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "\n",
    "                if isinstance(element, note.Note):\n",
    "                    out.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    out.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        with open('notes', 'wb') as filepath:\n",
    "            pickle.dump(out, filepath)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_uniq_notes(self):\n",
    "        note_counts = Counter(self.notes)\n",
    "        return sorted(note_counts, key=note_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.notes_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.notes_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.notes_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_notes)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(dataset, model, device, batch_size, sequence_length, max_epochs):\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in trange(max_epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item)\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print({ '###  epoch': epoch+1, 'batch': batch, 'loss': loss.item() })\n",
    "\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(dataset, model, start, next_notes=100):\n",
    "    model.eval()\n",
    "\n",
    "    notes = start.split(' ')\n",
    "    state_h, state_c = model.init_state(len(notes))\n",
    "\n",
    "    for i in range(0, next_notes):\n",
    "        x = torch.tensor([[dataset.note_to_index[w] for w in notes[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_note_logits = y_pred[0][-1]\n",
    "        p = torch.nn.Softmax(last_note_logits).detach().numpy()\n",
    "        note_index = np.random.choice(len(last_note_logits), p=p)\n",
    "        notes.append(dataset.index_to_note[note_index])\n",
    "\n",
    "    return notes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sequence_length = 32\n",
    "dataset = Dataset(sequence_length= sequence_length, file_limit=80)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LSTMNetwork(dataset).to(device)\n",
    "print(model)\n",
    "\n",
    "train_losses = train(dataset, model, device, batch_size=256, max_epochs=10, sequence_length= sequence_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    return midi_stream"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "music = predict(dataset, model, start='A1 B5 B4 A1')\n",
    "generated_stream = create_midi(music)\n",
    "generated_stream.write('midi', fp='output/uno.midi')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
