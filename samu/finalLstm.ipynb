{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# final version\n",
    "\n",
    "LSTM that can produce music training with a midi dataset\n",
    "\n",
    "requirements and instructions for Giuseppe:\n",
    "* anaconda\n",
    "* cuda for GPU integration\n",
    "* pytorch from pytorch repo(```conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia```  if im not mistaking)\n",
    "* tqdm and music21 from conda-forge(```conda install -c conda-forge tqdm music21```)\n",
    "\n",
    "\n",
    "There are some saved file, a 100 songs preloaded dataset(in the dataset loading cell there are more instuctions),\n",
    "a 10 epoch pretrained model with this dataset, more instructions in the related cells.\n",
    "\n",
    "\n",
    "For song generation you can see the code in the end, there is a function that can take the first N notes from a requested song in the dataset.\n",
    "\n",
    "\n",
    "[main credits](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html)\n",
    "\n",
    "\n",
    "\n",
    "> this file is enterely written and developed by Samuele Facenda(with the help of a lot of tutorial in the web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#from tqdm.notebook import tqdm, trange\n",
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from music21 import *\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "import pyperclip\n",
    "DIR = '../midiFiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## caricatore e lettore di dati, direttamente in un dataset pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            sequence_length,\n",
    "            file_limit,\n",
    "            reload_from_save=False\n",
    "    ):\n",
    "        if reload_from_save:\n",
    "            self.reload_from_save()\n",
    "            return\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.notes = self.load_notes(file_limit)\n",
    "        self.uniq_notes = self.get_uniq_notes()\n",
    "\n",
    "        self.index_to_note = {index: note  for index, note in enumerate(self.uniq_notes)}\n",
    "        self.note_to_index = {note: index for index, note in enumerate(self.uniq_notes)}\n",
    "\n",
    "        self.notes_indexes = np.array([self.note_to_index[w] for w in self.notes], dtype='int64')\n",
    "\n",
    "    def load_notes(self, file_limit):\n",
    "        out = []\n",
    "        if file_limit is None:\n",
    "            file_limit = len(listdir(DIR))\n",
    "\n",
    "        for file in tqdm(listdir(DIR), total=file_limit):\n",
    "            if not \"mid\" in file:\n",
    "                continue\n",
    "\n",
    "            if file_limit <= 0:\n",
    "                break\n",
    "            else:\n",
    "                file_limit -= 1\n",
    "\n",
    "            midifile = converter.parse(DIR + '/' + file)\n",
    "\n",
    "            #ho solo il piano\n",
    "            notes_to_parse = midifile.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "\n",
    "                if isinstance(element, note.Note):\n",
    "                    out.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    out.append('.'.join(str(n) for n in element.normalOrder))\n",
    "        return out\n",
    "\n",
    "    def get_uniq_notes(self):\n",
    "        note_counts = Counter(self.notes)\n",
    "        return sorted(note_counts, key=note_counts.get, reverse=True)\n",
    "\n",
    "    def save(self):\n",
    "        path_pre = 'saves/'\n",
    "        to_save = [self.sequence_length, self.notes, self.uniq_notes, self.index_to_note, self.note_to_index, self.notes_indexes]\n",
    "        paths = ['length', 'notes', 'uniques', 'index_dict', 'note_dict', 'indexes']\n",
    "\n",
    "        for var, path in zip(to_save, paths):\n",
    "            with open(path_pre + path + '.pickle', 'wb') as filepath:\n",
    "                pickle.dump(var, filepath)\n",
    "\n",
    "\n",
    "    def reload_from_save(self):\n",
    "        path_pre = 'saves/'\n",
    "        to_save = ['sequence_length', 'notes', 'uniq_notes', 'index_to_note', 'note_to_index', 'notes_indexes']\n",
    "        paths = ['length', 'notes', 'uniques', 'index_dict', 'note_dict', 'indexes']\n",
    "\n",
    "        for var, path in zip(to_save, paths):\n",
    "            with open(path_pre + path + '.pickle', 'rb') as filepath:\n",
    "                value = pickle.load(filepath)\n",
    "                setattr(self, var, value)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.notes_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.notes_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.notes_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rete neurale LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_notes)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length, device=\"cpu\"):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataset, model, device, batch_size, sequence_length, max_epochs):\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in trange(max_epochs, leave=True):\n",
    "        state_h, state_c = model.init_state(sequence_length, device)\n",
    "\n",
    "        for batch, (x, y) in tqdm(enumerate(dataloader), leave=False, position=1, total=len(dataloader)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                pass\n",
    "                #print({ '###  epoch': epoch+1, 'batch': batch, 'loss': loss.item() })\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "funzione per generare una serie di note data una serie di partenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(dataset, model, start, next_notes=100):\n",
    "    model.eval()\n",
    "\n",
    "    notes = start.split(' ')\n",
    "    state_h, state_c = model.init_state(len(notes))\n",
    "\n",
    "    for i in range(0, next_notes):\n",
    "        x = torch.tensor([[dataset.note_to_index[w] for w in notes[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_note_logits = y_pred[0][-1]\n",
    "        p = torch.nn.Softmax()(last_note_logits).detach().numpy()\n",
    "        note_index = np.random.choice(len(last_note_logits), p=p)\n",
    "        notes.append(dataset.index_to_note[note_index])\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loading\n",
    "\n",
    "set the reload variable at true if you want to not load the files but load the dumped dataset. By changing file limit you select the number of file to load from the dataset(None is all the dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length = 32\n",
    "reload = False\n",
    "dataset = Dataset(sequence_length= sequence_length, file_limit=30, reload_from_save=reload)\n",
    "if not reload:\n",
    "    # save to disk\n",
    "    dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "change max_epoch for a more intensive training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMNetwork(dataset).to(device)\n",
    "print(model)\n",
    "\n",
    "train_losses = train(dataset, model, device, batch_size=256, max_epochs=5, sequence_length= sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Grafico delle loss durante il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "funzioni comode per salvare e caricare il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, number=0):\n",
    "    with open('saves/modello' + str(number) + '.pickle', 'wb') as filepath:\n",
    "        pickle.dump(model, filepath)\n",
    "\n",
    "def load_model():\n",
    "    with open('saves/modello' + '.pickle', 'rb') as filepath:\n",
    "        return pickle.load(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model saving and reloading\n",
    "\n",
    "call save_model with a number to save the model in a file with the number in the title(to not override other models),\n",
    "call load_model with a number to reload the model with that number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_model(model, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, strumento):\n",
    "    if strumento is None:\n",
    "        strumento = instrument.Piano()\n",
    "\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = strumento\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = strumento\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Midi generation, song generation\n",
    "\n",
    "change the start string in the predict function to obtain different result, you can copy them from the files with the following function, example above. The instrument changing does not work. Change the name in the fo(filepath) attribute in the generated_stream.write function to not ovveride other files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "music = predict(dataset, model, start='C#3 1.5 8.1 1.5 1.6 E-3 1.6 1.6 C#5 8.1 1.5 G#5 1 8.1 10.1 1.6')\n",
    "generated_stream = create_midi(music, instrument.Banjo())#non sono capace di cambiare lo strumento\n",
    "path = generated_stream.write('midi', fp='./saves/midi/cinque.midi')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "prende le prime tot note da un pezzo dato, volendo copia nella clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_notes(file_number, n_notes, copy_to_clipboard=False):\n",
    "    midifile = converter.parse('../midiFiles/track' + str(file_number) + '.midi')\n",
    "\n",
    "    #ho solo il piano\n",
    "    notes_to_parse = midifile.flat.notes\n",
    "    out=[]\n",
    "    for element in notes_to_parse:\n",
    "\n",
    "        if n_notes <= 0:\n",
    "            break\n",
    "        else:\n",
    "            n_notes -= 1\n",
    "\n",
    "        if isinstance(element, note.Note):\n",
    "            out.append(str(element.pitch))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            out.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    out = ' '.join(out)\n",
    "    print(out)\n",
    "    if copy_to_clipboard:\n",
    "        pyperclip.copy(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_notes(file_number=600, n_notes=16, copy_to_clipboard=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
